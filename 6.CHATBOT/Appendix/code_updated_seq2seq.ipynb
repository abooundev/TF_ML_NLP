{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/csg/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "#    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#    tf.app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "#import tensorflow as tf\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel') # 주피터에서 커널에 전달하기 위한 프레그 방법\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, 'batch size') # 배치 크기\n",
    "tf.app.flags.DEFINE_integer('train_steps', 20000, 'train steps') # 학습 에포크\n",
    "tf.app.flags.DEFINE_float('dropout_width', 0.5, 'dropout width') # 드롭아웃 크기\n",
    "tf.app.flags.DEFINE_integer('layer_size', 3, 'layer size') # 멀티 레이어 크기 (multi rnn)\n",
    "tf.app.flags.DEFINE_integer('hidden_size', 128, 'weights size') # 가중치 크기\n",
    "tf.app.flags.DEFINE_float('learning_rate', 1e-3, 'learning rate') # 학습률\n",
    "tf.app.flags.DEFINE_string('data_path', './../data_in/ChatBotData.csv', 'data path') #  데이터 위치\n",
    "tf.app.flags.DEFINE_string('vocabulary_path', './data_out/vocabularyData.voc', 'vocabulary path') # 사전 위치\n",
    "tf.app.flags.DEFINE_string('check_point_path', './data_out/check_point', 'check point path') # 체크 포인트 위치\n",
    "tf.app.flags.DEFINE_integer('shuffle_seek', 1000, 'shuffle random seek') # 셔플 시드값\n",
    "tf.app.flags.DEFINE_integer('max_sequence_length', 25, 'max sequence length') # 시퀀스 길이\n",
    "tf.app.flags.DEFINE_integer('embedding_size', 128, 'embedding size') # 임베딩 크기\n",
    "tf.app.flags.DEFINE_boolean('tokenize_as_morph', True, 'set morph tokenize') # 형태소에 따른 토크나이징 사용 유무\n",
    "tf.app.flags.DEFINE_boolean('embedding', True, 'Use Embedding flag') # 임베딩 유무 설정\n",
    "tf.app.flags.DEFINE_boolean('multilayer', True, 'Use Multi RNN Cell') # 멀티 RNN 유무\n",
    "# Define FLAGS\n",
    "DEFINES = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "import enum\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#from configs import DEFINES\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "PAD = \"<PADDING>\"\n",
    "STD = \"<START>\"\n",
    "END = \"<END>\"\n",
    "UNK = \"<UNKNOWN>\"\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]\n",
    "CHANGE_FILTER = re.compile(FILTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 판다스를 통해서 데이터를 불러와 학습 셋과 평가 셋으로\n",
    "# 나누어 그 값을 리턴한다.\n",
    "def load_data():\n",
    "    data_df = pd.read_csv(DEFINES.data_path, header=0)\n",
    "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "    train_input, eval_input, train_label, eval_label = train_test_split(question, answer, test_size=0.33, random_state=42)\n",
    "    return train_input, train_label, eval_input, eval_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okt.morphs 함수를 통해 토크나이즈 된 \n",
    "# 리스트 객체를 받아 문자열을 재구성해서 리턴한다.\n",
    "def prepro_like_morphlized(data):\n",
    "    morph_analyzer = Okt()\n",
    "    result_data = list()\n",
    "    for seq in tqdm(data):\n",
    "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
    "        result_data.append(morphlized_seq)\n",
    "\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩 데이터를 만드는 함수이며 \n",
    "# 인덱스화 할 value와 키가 단어이고 값이 인덱스인 딕셔너리를 받아\n",
    "# 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.  \n",
    "def enc_processing(value, dictionary):\n",
    "    sequences_input_index = []\n",
    "    sequences_length = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        sequence_index = []\n",
    "        # 문장을 스페이스 단위로 자르고 있다.\n",
    "        for word in sequence.split():\n",
    "            # 잘려진 단어들이 딕셔너리에 존재 하는지 보고 \n",
    "            # 그 값을 가져와 sequence_index에 추가한다.\n",
    "            if dictionary.get(word) is not None:\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            # 잘려진 단어가 딕셔너리에 존재 하지 않는 \n",
    "            # 경우 이므로 UNK(2)를 넣어 준다.\n",
    "            else:\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "        \n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        if len(sequence_index) > DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n",
    "\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        # max_sequence_length보다 문장 길이가 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_input_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다. \n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
    "    return np.asarray(sequences_input_index), sequences_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코딩 입력 데이터를 만드는 함수이다.\n",
    "def dec_input_processing(value, dictionary):\n",
    "    sequences_output_index = []\n",
    "    sequences_length = []\n",
    "\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        sequence_index = []\n",
    "        # 디코딩 입력의 처음에는 START가 와야 하므로 \n",
    "        # 그 값을 넣어 주고 시작한다.\n",
    "        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n",
    "\n",
    "        if len(sequence_index) > DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_output_index.append(sequence_index)\n",
    "\n",
    "    return np.asarray(sequences_output_index), sequences_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코딩 출력 데이터를 만드는 함수이다.\n",
    "def dec_target_processing(value, dictionary):\n",
    "    sequences_target_index = []\n",
    "\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "    for sequence in value:\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 문장에서 스페이스 단위별로 단어를 가져와서 \n",
    "        # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
    "        # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
    "        sequence_index = [dictionary[word] for word in sequence.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        # 그리고 END 토큰을 넣어 준다\n",
    "        if len(sequence_index) >= DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length-1] + [dictionary[END]]\n",
    "        else:\n",
    "            sequence_index += [dictionary[END]]\n",
    "        # max_sequence_length보다 문장 길이가 \n",
    "\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_target_index.append(sequence_index)\n",
    "\n",
    "    return np.asarray(sequences_target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 스트링으로 변경하는 함수이다.\n",
    "def pred2string(value, dictionary):\n",
    "    sentence_string = []\n",
    "    for v in value:\n",
    "        # 딕셔너리에 있는 단어로 변경해서 배열에 담는다.\n",
    "        sentence_string = [dictionary[index] for index in v['indexs']]\n",
    "\n",
    "    print(sentence_string)\n",
    "    answer = \"\"\n",
    "    # 패딩값과 엔드값이 담겨 있으므로 패딩은 모두 스페이스 처리 한다.\n",
    "    for word in sentence_string:\n",
    "        if word not in PAD and word not in END:\n",
    "            answer += word\n",
    "            answer += \" \"\n",
    "\n",
    "    print(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 각 요소에 대해서 rearrange 함수를 \n",
    "# 통해서 요소를 변환하여 맵으로 구성한다.\n",
    "def rearrange(input, output, target):\n",
    "    features = {\"input\": input, \"output\": output}\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 들어가 배치 데이터를 만드는 함수이다.\n",
    "def train_input_fn(train_input_enc, train_output_dec, train_target_dec, batch_size):\n",
    "    # Dataset을 생성하는 부분으로써 from_tensor_slices부분은 \n",
    "    # 각각 한 문장으로 자른다고 보면 된다.\n",
    "    # train_input_enc, train_output_dec, train_target_dec \n",
    "    # 3개를 각각 한문장으로 나눈다.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_input_enc, train_output_dec, train_target_dec))\n",
    "    dataset = dataset.shuffle(buffer_size=len(train_input_enc))\n",
    "    # 배치 인자 값이 없다면  에러를 발생 시킨다.\n",
    "    assert batch_size is not None, \"train batchSize must not be None\"\n",
    "    # from_tensor_slices를 통해 나눈것을 배치크기 만큼 묶어 준다.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(rearrange)\n",
    "    # repeat()함수에 원하는 에포크 수를 넣을수 있으면 \n",
    "    # 아무 인자도 없다면 무한으로 이터레이터 된다.\n",
    "    dataset = dataset.repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    # 이터레이터를 통해 다음 항목의 텐서 개체를 넘겨준다.\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가에 들어가 배치 데이터를 만드는 함수이다.\n",
    "def eval_input_fn(eval_input_enc, eval_output_dec, eval_target_dec, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eval_input_enc, eval_output_dec, eval_target_dec))\n",
    "    # 전체 데이터를 섞는다.\n",
    "    dataset = dataset.shuffle(buffer_size=len(eval_input_enc))\n",
    "    assert batch_size is not None, \"eval batchSize must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(rearrange)\n",
    "    # 평가이므로 1회만 동작 시킨다.\n",
    "    dataset = dataset.repeat(1)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이징 해서 담을 배열을 생성하고 \n",
    "# 토그나이징과 정규표현식을 통해 만들어진 값들을 넘겨 준다.\n",
    "def data_tokenizer(data):\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        for word in sentence.split():\n",
    "            words.append(word)\n",
    "    return [word for word in words if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최초 사전 파일을 만드는 함수이며 파일이 존재 한다면 불러오는 함수이다.\n",
    "def load_vocabulary():\n",
    "    vocabulary_list = []\n",
    "    # 사전 파일의 존재 유무를 확인한다.\n",
    "    if (not (os.path.exists(DEFINES.vocabulary_path))):\n",
    "        if (os.path.exists(DEFINES.data_path)):\n",
    "            data_df = pd.read_csv(DEFINES.data_path, encoding='utf-8')\n",
    "            question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "            if DEFINES.tokenize_as_morph:  \n",
    "                question = prepro_like_morphlized(question)\n",
    "                answer = prepro_like_morphlized(answer)\n",
    "            data = []\n",
    "            data.extend(question)\n",
    "            data.extend(answer)\n",
    "            words = data_tokenizer(data)\n",
    "            words = list(set(words))\n",
    "            # 데이터 없는 내용중에 MARKER를 사전에 \n",
    "            # 추가 하기 위해서 아래와 같이 처리 한다.\n",
    "            # 아래는 MARKER 값이며 리스트의 첫번째 부터 \n",
    "            # 순서대로 넣기 위해서 인덱스 0에 추가한다.\n",
    "            # PAD = \"<PADDING>\"\n",
    "            # STD = \"<START>\"\n",
    "            # END = \"<END>\"\n",
    "            # UNK = \"<UNKNOWN>\"     \n",
    "            words[:0] = MARKER\n",
    "        # 사전 리스트를 사전 파일로 만들어 넣는다.\n",
    "        with open(DEFINES.vocabulary_path, 'w', encoding='utf-8') as vocabulary_file:\n",
    "            for word in words:\n",
    "                vocabulary_file.write(word + '\\n')\n",
    "\n",
    "    # 사전 파일이 존재하면 여기에서 그 파일을 불러서 배열에 넣어 준다.\n",
    "    with open(DEFINES.vocabulary_path, 'r', encoding='utf-8') as vocabulary_file:\n",
    "        for line in vocabulary_file:\n",
    "            vocabulary_list.append(line.strip())\n",
    "\n",
    "    word2idx, idx2word = make_vocabulary(vocabulary_list)\n",
    "    # 두가지 형태의 키와 값이 있는 형태를 리턴한다. \n",
    "    # (예) 단어: 인덱스 , 인덱스: 단어)\n",
    "    return word2idx, idx2word, len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트를 키가 단어이고 값이 인덱스인 딕셔너리를 만든다.\n",
    "# 리스트를 키가 인덱스이고 값이 단어인 딕셔너리를 만든다.\n",
    "def make_vocabulary(vocabulary_list):\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocabulary_list)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(vocabulary_list)}\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_data(self):\n",
    "    char2idx, idx2char, vocabulary_length = load_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#import tensorflow as tf\n",
    "import sys\n",
    "import numpy as np\n",
    "#from configs import DEFINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_cell(mode, hiddenSize, index):\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(hiddenSize, name=\"lstm\" + str(index), state_is_tuple=False)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, state_keep_prob=DEFINES.dropout_width)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에스티메이터 모델 부분이다.\n",
    "# freatures : tf.data.Dataset.map을 통해서 만들어진 \n",
    "# features = {\"input\": input, \"length\": length}\n",
    "# labels : tf.data.Dataset.map을 통해서 만들어진 target\n",
    "# mode는 에스티메이터 함수를 호출하면 에스티메이터 \n",
    "# 프레임워크 모드의 값이 해당 부분이다.\n",
    "# params : 에스티메이터를 구성할때 params 값들이다. \n",
    "# (params={ # 모델 쪽으로 파라메터 전달한다.)\n",
    "def Model(features, labels, mode, params):\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "\n",
    "    # 미리 정의된  임베딩 사용 유무를 확인 한다.\n",
    "    # 값이 True이면 임베딩을 해서 학습하고 False이면 \n",
    "    # onehotencoding 처리 한다.\n",
    "    if params['embedding'] == True:\n",
    "        # 가중치 행렬에 대한 초기화 함수이다.\n",
    "        # xavier (Xavier Glorot와 Yoshua Bengio (2010)\n",
    "        # URL : http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        # 인코딩 변수를 선언하고 값을 설정한다.\n",
    "        embedding_encoder = tf.get_variable(name=\"embedding_encoder\",  # 이름\n",
    "                                           shape=[params['vocabulary_length'], params['embedding_size']],  # 모양\n",
    "                                           dtype=tf.float32,  # 타입\n",
    "                                           initializer=initializer,  # 초기화 값\n",
    "                                           trainable=True)  # 학습 유무\n",
    "    else:\n",
    "        # tf.eye를 통해서 사전의 크기 만큼의 단위행렬 \n",
    "        # 구조를 만든다.\n",
    "        embedding_encoder = tf.eye(num_rows=params['vocabulary_length'], dtype=tf.float32)\n",
    "        # 인코딩 변수를 선언하고 값을 설정한다.\n",
    "        embedding_encoder = tf.get_variable(name=\"embedding_encoder\",  # 이름\n",
    "                                           initializer=embedding_encoder,  # 초기화 값\n",
    "                                           trainable=False)  # 학습 유무\n",
    "\n",
    "    # embedding_lookup을 통해서 features['input']의 인덱스를\n",
    "    # 위에서 만든 embedding_encoder의 인덱스의 값으로 변경하여 \n",
    "    # 임베딩된 디코딩 배치를 만든다.\n",
    "    embedding_encoder_batch = tf.nn.embedding_lookup(params=embedding_encoder, ids=features['input'])\n",
    "\n",
    "    # 미리 정의된  임베딩 사용 유무를 확인 한다.\n",
    "    # 값이 True이면 임베딩을 해서 학습하고 False이면 \n",
    "    # onehotencoding 처리 한다.\n",
    "    if params['embedding'] == True:\n",
    "        # 가중치 행렬에 대한 초기화 함수이다.\n",
    "        # xavier (Xavier Glorot와 Yoshua Bengio (2010)\n",
    "        # URL : http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        # 디코딩 변수를 선언하고 값을 설정한다.\n",
    "        embedding_decoder = tf.get_variable(name=\"embedding_decoder\",  # 이름\n",
    "                                           shape=[params['vocabulary_length'], params['embedding_size']],  # 모양\n",
    "                                           dtype=tf.float32,  # 타입\n",
    "                                           initializer=initializer,  # 초기화 값\n",
    "                                           trainable=True)  # 학습 유무\n",
    "    else:\n",
    "        # tf.eye를 통해서 사전의 크기 만큼의 단위행렬 \n",
    "        # 구조를 만든다.\n",
    "        embedding_decoder = tf.eye(num_rows=params['vocabulary_length'], dtype=tf.float32)\n",
    "        # 인코딩 변수를 선언하고 값을 설정한다.\n",
    "        embedding_decoder = tf.get_variable(name='embedding_decoder',  # 이름\n",
    "                                           initializer=embedding_decoder,  # 초기화 값\n",
    "                                           trainable=False)  # 학습 유무\n",
    "\n",
    "    # 변수 재사용을 위해서 reuse=.AUTO_REUSE를 사용하며 범위를\n",
    "    # 정해주고 사용하기 위해 scope설정을 한다.\n",
    "    # make_lstm_cell이 \"cell\"반복적으로 호출 되면서 재사용된다.\n",
    "    with tf.variable_scope('encoder_scope', reuse=tf.AUTO_REUSE):\n",
    "        # 값이 True이면 멀티레이어로 모델을 구성하고 False이면 \n",
    "        # 단일레이어로 모델을 구성 한다.\n",
    "        if params['multilayer'] == True:\n",
    "            # layerSize 만큼  LSTMCell을  encoder_cell_list에 담는다.\n",
    "            encoder_cell_list = [make_lstm_cell(mode, params['hidden_size'], i) for i in range(params['layer_size'])]\n",
    "            # MUltiLayer RNN CEll에 encoder_cell_list를 넣어 멀티 레이어를 만든다.\n",
    "            rnn_cell = tf.contrib.rnn.MultiRNNCell(encoder_cell_list, state_is_tuple=False)\n",
    "        else:\n",
    "            # 단층 LSTMLCell을 만든다.\n",
    "            rnn_cell = make_lstm_cell(mode, params['hidden_size'], \"\")\n",
    "        # rnn_cell에 의해 지정된 반복적인 신경망을 만든다.\n",
    "        # encoder_outputs(RNN 출력 Tensor)[batch_size, \n",
    "        # max_time, cell.output_size]\n",
    "        # encoder_states 최종 상태  [batch_size, cell.state_size]\n",
    "        encoder_outputs, encoder_states = tf.nn.dynamic_rnn(cell=rnn_cell,  # RNN 셀\n",
    "                                                              inputs=embedding_encoder_batch,  # 입력 값\n",
    "                                                              dtype=tf.float32)  # 타입\n",
    "        # 변수 재사용을 위해서 reuse=.AUTO_REUSE를 사용하며 범위를 정해주고\n",
    "        # 사용하기 위해 scope설정을 한다.\n",
    "        # make_lstm_cell이 \"cell\"반복적으로 호출 되면서 재사용된다.\n",
    "    with tf.variable_scope('decoder_scope', reuse=tf.AUTO_REUSE):\n",
    "        # 값이 True이면 멀티레이어로 모델을 구성하고 False이면 단일레이어로\n",
    "        # 모델을 구성 한다.\n",
    "        if params['multilayer'] == True:\n",
    "            # layer_size 만큼  LSTMCell을  decoder_cell_list에 담는다.\n",
    "            decoder_cell_list = [make_lstm_cell(mode, params['hidden_size'], i) for i in range(params['layer_size'])]\n",
    "            # MUltiLayer RNN CEll에 decoder_cell_list를 넣어 멀티 레이어를 만든다.\n",
    "            rnn_cell = tf.contrib.rnn.MultiRNNCell(decoder_cell_list, state_is_tuple=False)\n",
    "        else:\n",
    "            # 단층 LSTMLCell을 만든다.\n",
    "            rnn_cell = make_lstm_cell(mode, params['hidden_size'], \"\")\n",
    "\n",
    "        decoder_state = encoder_states\n",
    "        # 매 타임 스텝에 나오는 아웃풋을 저장하는 리스트 두개를 만든다. \n",
    "        # 하나는 토큰 인덱스는 predict_tokens 저장\n",
    "        # 다른 하나는 temp_logits에 logits 저장한다.\n",
    "        predict_tokens = list()\n",
    "        temp_logits = list()\n",
    "\n",
    "        # 평가인 경우에는 teacher forcing이 되지 않도록 해야한다.\n",
    "        # 따라서 학습이 아닌경우에 is_train을 False로 하여 teacher forcing이 되지 않도록 한다.\n",
    "        output_token = tf.ones(shape=(tf.shape(encoder_outputs)[0],), dtype=tf.int32) * 1\n",
    "        # 전체 문장 길이 만큼 타임 스텝을 돌도록 한다.\n",
    "        for i in range(DEFINES.max_sequence_length):\n",
    "            # 두 번쨰 스텝 이후에는 teacher forcing을 적용하는지 확률에 따라 결정하도록 한다.\n",
    "            # teacher forcing rate은 teacher forcing을 어느정도 줄 것인지를 조절한다.\n",
    "            if TRAIN:\n",
    "                if i > 0:\n",
    "                    # tf.cond를 통해 rnn에 입력할 입력 임베딩 벡터를 결정한다 여기서 true인 경우엔 입력된 output값 아닌경우에는 이전 스텝에\n",
    "                    # 나온 output을 사용한다.\n",
    "                    input_token_emb = tf.cond(\n",
    "                        tf.logical_and( # 논리 and 연산자\n",
    "                            True,\n",
    "                            tf.random_uniform(shape=(), maxval=1) <= params['teacher_forcing_rate'] # 률에 따른 labels값 지원 유무\n",
    "                        ),\n",
    "                        lambda: tf.nn.embedding_lookup(embedding_decoder, labels[:, i-1]),  # labels 정답을 넣어주고 있다.\n",
    "                        lambda: tf.nn.embedding_lookup(embedding_decoder, output_token) # 모델이 정답이라고 생각 하는 값\n",
    "                    )\n",
    "                else:\n",
    "                    input_token_emb = tf.nn.embedding_lookup(embedding_decoder, output_token) # 모델이 정답이라고 생각 하는 값\n",
    "            else: # 평가 및 예측은 여기를 진행해야 한다. \n",
    "                input_token_emb = tf.nn.embedding_lookup(embedding_decoder, output_token)\n",
    "\n",
    "            # 어텐션 적용 부분\n",
    "            if params['attention'] == True:\n",
    "                W1 = tf.keras.layers.Dense(params['hidden_size'])\n",
    "                W2 = tf.keras.layers.Dense(params['hidden_size'])\n",
    "                V = tf.keras.layers.Dense(1)\n",
    "                # (?, 256) -> (?, 128)\n",
    "                hidden_with_time_axis = W2(decoder_state)\n",
    "                # (?, 128) -> (?, 1, 128)\n",
    "                hidden_with_time_axis = tf.expand_dims(hidden_with_time_axis, axis=1)\n",
    "                # (?, 1, 128) -> (?, 25, 128)\n",
    "                hidden_with_time_axis = tf.manip.tile(hidden_with_time_axis, [1, DEFINES.max_sequence_length, 1])\n",
    "                # (?, 25, 1)\n",
    "                score = V(tf.nn.tanh(W1(encoder_outputs) + hidden_with_time_axis))\n",
    "                # score = V(tf.nn.tanh(W1(encoderOutputs) + tf.manip.tile(tf.expand_dims(W2(decoder_state), axis=1), [1, DEFINES.maxSequenceLength, 1])))\n",
    "                # (?, 25, 1)\n",
    "                attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "                # (?, 25, 128)\n",
    "                context_vector = attention_weights * encoder_outputs\n",
    "                # (?, 25, 128) -> (?, 128)\n",
    "                context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "                # (?, 256)\n",
    "                input_token_emb = tf.concat([context_vector, input_token_emb], axis=-1)\n",
    "\n",
    "            # RNNCell을 호출하여 RNN 스텝 연산을 진행하도록 한다.\n",
    "            input_token_emb = tf.keras.layers.Dropout(0.5)(input_token_emb)\n",
    "            decoder_outputs, decoder_state = rnn_cell(input_token_emb, decoder_state)\n",
    "            decoder_outputs = tf.keras.layers.Dropout(0.5)(decoder_outputs)\n",
    "            # feedforward를 거쳐 output에 대한 logit값을 구한다.\n",
    "            output_logits = tf.layers.dense(decoder_outputs, params['vocabulary_length'], activation=None)\n",
    "\n",
    "            # softmax를 통해 단어에 대한 예측 probability를 구한다.\n",
    "            output_probs = tf.nn.softmax(output_logits)\n",
    "            output_token = tf.argmax(output_probs, axis=-1)\n",
    "\n",
    "            # 한 스텝에 나온 토큰과 logit 결과를 저장해둔다.\n",
    "            predict_tokens.append(output_token)\n",
    "            temp_logits.append(output_logits)\n",
    "\n",
    "        # 저장했던 토큰과 logit 리스트를 stack을 통해 메트릭스로 만들어 준다.\n",
    "        # 만들게 뙤면 차원이 [시퀀스 X 배치 X 단어 feature 수] 이렇게 되는데\n",
    "        # 이를 transpose하여 [배치 X 시퀀스 X 단어 feature 수] 로 맞춰준다.\n",
    "        predict = tf.transpose(tf.stack(predict_tokens, axis=0), [1, 0])\n",
    "        logits = tf.transpose(tf.stack(temp_logits, axis=0), [1, 0, 2])\n",
    "\n",
    "        print(predict.shape)\n",
    "        print(logits.shape)\n",
    "\n",
    "    if PREDICT:\n",
    "        if params['serving'] == True:\n",
    "            export_outputs = {\n",
    "                'indexs': tf.estimator.export.PredictOutput(predict) # 서빙 결과값을 준다.\n",
    "            }\n",
    "\n",
    "        predictions = {  # 예측 값들이 여기에 딕셔너리 형태로 담긴다.\n",
    "            'indexs': predict,  # 시퀀스 마다 예측한 값\n",
    "            'logits': logits,  # 마지막 결과 값\n",
    "        }\n",
    "        # 에스티메이터에서 리턴하는 값은 tf.estimator.EstimatorSpec \n",
    "        # 객체를 리턴 한다.\n",
    "        # mode : 에스티메이터가 수행하는 mode (tf.estimator.ModeKeys.PREDICT)\n",
    "        # predictions : 예측 값\n",
    "        if params['serving'] == True:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions, export_outputs=export_outputs)\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # 마지막 결과 값과 정답 값을 비교하는 \n",
    "    # tf.nn.sparse_softmax_cross_entropy_with_logits(로스함수)를 \n",
    "    # 통과 시켜 틀린 만큼의\n",
    "    # 에러 값을 가져 오고 이것들은 차원 축소를 통해 단일 텐서 값을 반환 한다.\n",
    "    # pad의 loss값을 무력화 시킨다. pad가 아닌값은 1 pad인 값은 0을 주어 동작\n",
    "    # 하도록 한다.\n",
    "    # 정답 차원 변경을 한다. [배치 * max_sequence_length * vocabulary_length]  \n",
    "    # logits과 같은 차원을 만들기 위함이다.\n",
    "    labels_ = tf.one_hot(labels, params['vocabulary_length'])\n",
    "    \n",
    "    if TRAIN and params['loss_mask'] == True:\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_))\n",
    "        masks = features['length']\n",
    "\n",
    "        loss = loss * tf.cast(masks, tf.float32)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "    else:\n",
    "       loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_))\n",
    "    # 라벨과 결과가 일치하는지 빈도 계산을 통해 \n",
    "    # 정확도를 측정하는 방법이다.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predict, name='accOp')\n",
    "\n",
    "    # 정확도를 전체값으로 나눈 값이다.\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    # 평가 mode 확인 부분이며 평가는 여기 까지 \n",
    "    # 수행하고 리턴한다.\n",
    "    if EVAL:\n",
    "        # 에스티메이터에서 리턴하는 값은 \n",
    "        # tf.estimator.EstimatorSpec 객체를 리턴 한다.\n",
    "        # mode : 에스티메이터가 수행하는 mode (tf.estimator.ModeKeys.EVAL)\n",
    "        # loss : 에러 값\n",
    "        # eval_metric_ops : 정확도 값\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # 파이썬 assert구문으로 거짓일 경우 프로그램이 종료 된다.\n",
    "    # 수행 mode(tf.estimator.ModeKeys.TRAIN)가 \n",
    "    # 아닌 경우는 여기 까지 오면 안되도록 방어적 코드를 넣은것이다.\n",
    "    assert TRAIN\n",
    "\n",
    "    # 아담 옵티마이저를 사용한다.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=DEFINES.learning_rate)\n",
    "    # 에러값을 옵티마이저를 사용해서 최소화 시킨다.\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    # 에스티메이터에서 리턴하는 값은 tf.estimator.EstimatorSpec 객체를 리턴 한다.\n",
    "    # mode : 에스티메이터가 수행하는 mode (tf.estimator.ModeKeys.EVAL)\n",
    "    # loss : 에러 값\n",
    "    # train_op : 그라디언트 반환\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#import model as ml\n",
    "#import data\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "#from configs import DEFINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serving 기능을 위하여 serving 함수를 구성한다.\n",
    "def serving_input_receiver_fn():\n",
    "    receiver_tensor = {\n",
    "        'input': tf.placeholder(dtype=tf.int32, shape=[None, DEFINES.max_sequence_length]),\n",
    "        'output': tf.placeholder(dtype=tf.int32, shape=[None, DEFINES.max_sequence_length])    \n",
    "    }\n",
    "    features = {\n",
    "        key: tensor for key, tensor in receiver_tensor.items()\n",
    "    }\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_out/'\n",
    "data_out_path = os.path.join(os.getcwd(), DATA_OUT_PATH)\n",
    "os.makedirs(data_out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11823/11823 [01:13<00:00, 160.56it/s]\n",
      "100%|██████████| 11823/11823 [01:46<00:00, 110.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 통한 사전 구성 한다.\n",
    "char2idx, idx2char, vocabulary_length = load_vocabulary() #data.load_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터와 테스트 데이터를 가져온다.\n",
    "train_input, train_label, eval_input, eval_label = load_data() #data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7921/7921 [00:40<00:00, 198.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# 훈련셋 인코딩 만드는 부분이다.\n",
    "train_input_enc, train_input_enc_length = enc_processing(train_input, char2idx) #data.enc_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7921/7921 [01:00<00:00, 131.22it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-d2f02a9c8e82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 훈련셋 디코딩 출력 부분 만드는 부분이다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_target_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_dec_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_target_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar2idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#data.dec_target_processing()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 훈련셋 디코딩 출력 부분 만드는 부분이다.\n",
    "train_target_dec, train_target_dec_length = dec_target_processing(train_label, char2idx) #data.dec_target_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3902/3902 [00:23<00:00, 164.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# 평가셋 인코딩 만드는 부분이다.\n",
    "eval_input_enc, eval_input_enc_length = enc_processing(eval_input,char2idx) #data.enc_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3902/3902 [00:55<00:00, 70.54it/s] \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-baabfff08d44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 평가셋 디코딩 출력 부분 만드는 부분이다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meval_target_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_target_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar2idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#data.dec_target_processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 평가셋 디코딩 출력 부분 만드는 부분이다.\n",
    "eval_target_dec, _ = dec_target_processing(eval_label, char2idx) #data.dec_target_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "save_model_path",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-411cab54a04a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 체크 포인트를 저장한 디렉토리를 설정한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheck_point_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFINES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_point_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msave_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFINES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# 디렉토리를 만드는 함수이며 두번째 인자 exist_ok가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# True이면 디렉토리가 이미 존재해도 OSError가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0mfl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__hiddenflags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: save_model_path"
     ]
    }
   ],
   "source": [
    "# 현재 경로'./'에 현재 경로 하부에 \n",
    "# 체크 포인트를 저장한 디렉토리를 설정한다.\n",
    "check_point_path = os.path.join(os.getcwd(), DEFINES.check_point_path)\n",
    "save_model_path = os.path.join(os.getcwd(), DEFINES.save_model_path)\n",
    "# 디렉토리를 만드는 함수이며 두번째 인자 exist_ok가 \n",
    "# True이면 디렉토리가 이미 존재해도 OSError가 \n",
    "# 발생하지 않는다.\n",
    "# exist_ok가 False이면 이미 존재하면 \n",
    "# OSError가 발생한다.\n",
    "os.makedirs(check_point_path, exist_ok=True)\n",
    "os.makedirs(save_model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에스티메이터 구성한다.\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=Model,  # 모델 등록한다. #ml.Model,\n",
    "    model_dir=DEFINES.check_point_path,  # 체크포인트 위치 등록한다.\n",
    "    params={  # 모델 쪽으로 파라메터 전달한다.\n",
    "        'hidden_size': DEFINES.hidden_size,  # 가중치 크기 설정한다.\n",
    "        'layer_size': DEFINES.layer_size,  # 멀티 레이어 층 개수를 설정한다.\n",
    "        'learning_rate': DEFINES.learning_rate,  # 학습율 설정한다.\n",
    "        'teacher_forcing_rate': DEFINES.teacher_forcing_rate, # 학습시 디코더 인풋 정답 지원율 설정\n",
    "        'vocabulary_length': vocabulary_length,  # 딕셔너리 크기를 설정한다.\n",
    "        'embedding_size': DEFINES.embedding_size,  # 임베딩 크기를 설정한다.\n",
    "        'embedding': DEFINES.embedding,  # 임베딩 사용 유무를 설정한다.\n",
    "        'multilayer': DEFINES.multilayer,  # 멀티 레이어 사용 유무를 설정한다.\n",
    "        'attention': DEFINES.attention, #  어텐션 지원 유무를 설정한다.\n",
    "        'teacher_forcing': DEFINES.teacher_forcing, # 학습시 디코더 인풋 정답 지원 유무 설정한다.\n",
    "        'loss_mask': DEFINES.loss_mask, # PAD에 대한 마스크를 통한 loss를 제한 한다.\n",
    "        'serving': DEFINES.serving # 모델 저장 및 serving 유무를 설정한다.\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 실행\n",
    "# data.train_input_fn()\n",
    "classifier.train(input_fn=lambda: train_input_fn(\n",
    "    train_input_enc, train_target_dec_length, train_target_dec, DEFINES.batch_size), steps=DEFINES.train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서빙 기능 유무에 따라 모델을 Save 한다.\n",
    "if DEFINES.serving == True:\n",
    "    save_model_path = classifier.export_savedmodel(\n",
    "        export_dir_base=DEFINES.save_model_path,\n",
    "        serving_input_receiver_fn=serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 실행\n",
    "# data.eval_input_fn()\n",
    "eval_result = classifier.evaluate(input_fn=lambda: eval_input_fn(\n",
    "    eval_input_enc,eval_target_dec, DEFINES.batch_size))\n",
    "print('\\nEVAL set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#import tensorflow as tf\n",
    "#import data\n",
    "import sys\n",
    "#import model as ml\n",
    "#from configs import DEFINES\n",
    "\t\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    arg_length = len(sys.argv)\n",
    "    \n",
    "    if(arg_length < 2):\n",
    "        raise Exception(\"Don't call us. We'll call you\")\n",
    "  \n",
    "    \n",
    "    # 데이터를 통한 사전 구성 한다.\n",
    "    char2idx,  idx2char, vocabulary_length = data.load_vocabulary()\n",
    "\n",
    "    # 테스트용 데이터 만드는 부분이다.\n",
    "    # 인코딩 부분 만든다.\n",
    "    input = \"\"\n",
    "    for i in sys.argv[1:]:\n",
    "        input += i \n",
    "        input += \" \"\n",
    "        \n",
    "    print(input)\n",
    "    predic_input_enc, predic_input_enc_length = data.enc_processing([input], char2idx)\n",
    "    # 학습 과정이 아니므로 디코딩 입력은 \n",
    "    # 존재하지 않는다.(구조를 맞추기 위해 넣는다.)\n",
    "    # 학습 과정이 아니므로 디코딩 출력 부분도 \n",
    "    # 존재하지 않는다.(구조를 맞추기 위해 넣는다.)\n",
    "    predic_target_dec, _ = data.dec_target_processing([\"\"], char2idx)\n",
    "\n",
    "    if DEFINES.serving == True:\n",
    "        # 모델이 저장된 위치를 넣어 준다.  export_dir\n",
    "        predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "            export_dir=\"/home/evo_mind/DeepLearning/NLP/Work/ChatBot2_Final/data_out/model/1541575161\"\n",
    "        )\n",
    "    else:\n",
    "        # 에스티메이터 구성한다.\n",
    "        classifier = tf.estimator.Estimator(\n",
    "                model_fn=ml.Model, # 모델 등록한다.\n",
    "                model_dir=DEFINES.check_point_path, # 체크포인트 위치 등록한다.\n",
    "                params={ # 모델 쪽으로 파라메터 전달한다.\n",
    "                    'hidden_size': DEFINES.hidden_size,  # 가중치 크기 설정한다.\n",
    "                    'layer_size': DEFINES.layer_size,  # 멀티 레이어 층 개수를 설정한다.\n",
    "                    'learning_rate': DEFINES.learning_rate,  # 학습율 설정한다.\n",
    "                    'teacher_forcing_rate': DEFINES.teacher_forcing_rate, # 학습시 디코더 인풋 정답 지원율 설정\n",
    "                    'vocabulary_length': vocabulary_length,  # 딕셔너리 크기를 설정한다.\n",
    "                    'embedding_size': DEFINES.embedding_size,  # 임베딩 크기를 설정한다.\n",
    "                    'embedding': DEFINES.embedding,  # 임베딩 사용 유무를 설정한다.\n",
    "                    'multilayer': DEFINES.multilayer,  # 멀티 레이어 사용 유무를 설정한다.\n",
    "                    'attention': DEFINES.attention, #  어텐션 지원 유무를 설정한다.\n",
    "                    'teacher_forcing': DEFINES.teacher_forcing, # 학습시 디코더 인풋 정답 지원 유무 설정한다.\n",
    "                    'loss_mask': DEFINES.loss_mask, # PAD에 대한 마스크를 통한 loss를 제한 한다.\n",
    "                    'serving': DEFINES.serving # 모델 저장 및 serving 유무를 설정한다.\n",
    "                })\n",
    "\n",
    "    if DEFINES.serving == True:\n",
    "        predictions = predictor_fn({'input':predic_input_enc, 'output':predic_target_dec})\n",
    "        data.pred2string(predictions, idx2char)\n",
    "    else:\n",
    "        # 예측을 하는 부분이다.\n",
    "        predictions = classifier.predict(\n",
    "            input_fn=lambda:data.eval_input_fn(predic_input_enc, predic_target_dec, DEFINES.batch_size))\n",
    "        # 예측한 값을 인지 할 수 있도록 \n",
    "        # 텍스트로 변경하는 부분이다.\n",
    "        data.pred2string(predictions, idx2char)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
