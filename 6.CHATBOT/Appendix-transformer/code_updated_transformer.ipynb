{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/csg/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "#    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "#    tf.app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#import tensorflow as tf\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel') # 주피터에서 커널에 전달하기 위한 프레그 방법\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, 'batch size')  # 배치 크기\n",
    "tf.app.flags.DEFINE_integer('train_steps', 20000, 'train steps')  # 학습 에포크\n",
    "tf.app.flags.DEFINE_float('dropout_width', 0.5, 'dropout width')  # 드롭아웃 크기\n",
    "tf.app.flags.DEFINE_integer('embedding_size', 128, 'embedding size')  # 가중치 크기 # 논문 512 사용\n",
    "tf.app.flags.DEFINE_float('learning_rate', 1e-3, 'learning rate')  # 학습률\n",
    "tf.app.flags.DEFINE_integer('shuffle_seek', 1000, 'shuffle random seek')  # 셔플 시드값\n",
    "tf.app.flags.DEFINE_integer('max_sequence_length', 25, 'max sequence length')  # 시퀀스 길이\n",
    "tf.app.flags.DEFINE_integer('model_hidden_size', 128, 'model weights size')  # 모델 가중치 크기\n",
    "tf.app.flags.DEFINE_integer('ffn_hidden_size', 512, 'ffn weights size')  # ffn 가중치 크기\n",
    "tf.app.flags.DEFINE_integer('attention_head_size', 4, 'attn head size')  # 멀티 헤드 크기\n",
    "tf.app.flags.DEFINE_integer('layer_size', 2, 'layer size')  # 논문은 6개 레이어이나 2개 사용 학습 속도 및 성능 튜닝\n",
    "tf.app.flags.DEFINE_string('data_path', '../data_in/ChatBotData.csv', 'data path')  # 데이터 위치\n",
    "tf.app.flags.DEFINE_string('vocabulary_path', './data_out/vocabularyData.voc', 'vocabulary path')  # 사전 위치\n",
    "tf.app.flags.DEFINE_string('check_point_path', './data_out/check_point', 'check point path')  # 체크 포인트 위치\n",
    "tf.app.flags.DEFINE_boolean('tokenize_as_morph', False, 'set morph tokenize')  # 형태소에 따른 토크나이징 사용 유무\n",
    "tf.app.flags.DEFINE_boolean('xavier_initializer', True, 'set xavier initializer')  # 형태소에 따른 토크나이징 사용 유무\n",
    "\n",
    "# Define FLAGS\n",
    "DEFINES = tf.app.flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "import enum\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#from configs import DEFINES\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "PAD = \"<PAD>\"\n",
    "STD = \"<SOS>\"\n",
    "END = \"<END>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]\n",
    "CHANGE_FILTER = re.compile(FILTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # 판다스를 통해서 데이터를 불러온다.\n",
    "    data_df = pd.read_csv(DEFINES.data_path, header=0)\n",
    "    # 질문과 답변 열을 가져와 question과 answer에 넣는다.\n",
    "    question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "    # skleran에서 지원하는 함수를 통해서 학습 셋과\n",
    "    # 테스트 셋을 나눈다.\n",
    "    train_input, eval_input, train_label, eval_label = train_test_split(question, answer, test_size=0.33,\n",
    "                                                                        random_state=42)\n",
    "    # 그 값을 리턴한다.\n",
    "    return train_input, train_label, eval_input, eval_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_like_morphlized(data):\n",
    "    # 형태소 분석 모듈 객체를\n",
    "    # 생성합니다.\n",
    "\n",
    "    morph_analyzer = Twitter()\n",
    "    # 형태소 토크나이즈 결과 문장을 받을\n",
    "    #  리스트를 생성합니다.\n",
    "    result_data = list()\n",
    "    # 데이터에 있는 매 문장에 대해 토크나이즈를\n",
    "    # 할 수 있도록 반복문을 선언합니다.\n",
    "    for seq in tqdm(data):\n",
    "        # Twitter.morphs 함수를 통해 토크나이즈 된\n",
    "        # 리스트 객체를 받고 다시 공백문자를 기준으로\n",
    "        # 하여 문자열로 재구성 해줍니다.\n",
    "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
    "        result_data.append(morphlized_seq)\n",
    "\n",
    "    return result_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스화 할 value와 키가 워드이고\n",
    "# 값이 인덱스인 딕셔너리를 받는다.\n",
    "def enc_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는\n",
    "    # 배열이다.(누적된다.)\n",
    "    sequences_input_index = []\n",
    "    # 하나의 인코딩 되는 문장의\n",
    "    # 길이를 가지고 있다.(누적된다.)\n",
    "    sequences_length = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는\n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 하나의 문장을 인코딩 할때\n",
    "        # 가지고 있기 위한 배열이다.\n",
    "        sequence_index = []\n",
    "        # 문장을 스페이스 단위로\n",
    "        # 자르고 있다.\n",
    "        for word in sequence.split():\n",
    "            # 잘려진 단어들이 딕셔너리에 존재 하는지 보고\n",
    "            # 그 값을 가져와 sequence_index에 추가한다.\n",
    "            if dictionary.get(word) is not None:\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            # 잘려진 단어가 딕셔너리에 존재 하지 않는\n",
    "            # 경우 이므로 UNK(2)를 넣어 준다.\n",
    "            else:\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        if len(sequence_index) > DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n",
    "        # 하나의 문장에 길이를 넣어주고 있다.\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        # max_sequence_length보다 문장 길이가\n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을\n",
    "        # sequences_input_index에 넣어 준다.\n",
    "        sequences_input_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
    "    # 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과\n",
    "    # 그 길이를 넘겨준다.\n",
    "    return np.asarray(sequences_input_index), sequences_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스화 할 value 키가 워드 이고 값이\n",
    "# 인덱스인 딕셔너리를 받는다.\n",
    "def dec_output_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는\n",
    "    # 배열이다.(누적된다)\n",
    "    sequences_output_index = []\n",
    "    # 하나의 디코딩 입력 되는 문장의\n",
    "    # 길이를 가지고 있다.(누적된다)\n",
    "    sequences_length = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는\n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 하나의 문장을 디코딩 할때 가지고\n",
    "        # 있기 위한 배열이다.\n",
    "        sequence_index = []\n",
    "        # 디코딩 입력의 처음에는 START가 와야 하므로\n",
    "        # 그 값을 넣어 주고 시작한다.\n",
    "        # 문장에서 스페이스 단위별로 단어를 가져와서 딕셔너리의\n",
    "        # 값인 인덱스를 넣어 준다.\n",
    "        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        if len(sequence_index) > DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length]\n",
    "        # 하나의 문장에 길이를 넣어주고 있다.\n",
    "        sequences_length.append(len(sequence_index))\n",
    "        # max_sequence_length보다 문장 길이가\n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을\n",
    "        # sequences_output_index 넣어 준다.\n",
    "        sequences_output_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한\n",
    "    # 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
    "    return np.asarray(sequences_output_index), sequences_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스화 할 value와 키가 워드 이고\n",
    "# 값이 인덱스인 딕셔너리를 받는다.\n",
    "def dec_target_processing(value, dictionary):\n",
    "    # 인덱스 값들을 가지고 있는\n",
    "    # 배열이다.(누적된다)\n",
    "    sequences_target_index = []\n",
    "    # 형태소 토크나이징 사용 유무\n",
    "    if DEFINES.tokenize_as_morph:\n",
    "        value = prepro_like_morphlized(value)\n",
    "    # 한줄씩 불어온다.\n",
    "    for sequence in value:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 정규화를 사용하여 필터에 들어 있는\n",
    "        # 값들을 \"\" 으로 치환 한다.\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        # 문장에서 스페이스 단위별로 단어를 가져와서\n",
    "        # 딕셔너리의 값인 인덱스를 넣어 준다.\n",
    "        # 디코딩 출력의 마지막에 END를 넣어 준다.\n",
    "        sequence_index = [dictionary[word] for word in sequence.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 자르고 있다.\n",
    "        # 그리고 END 토큰을 넣어 준다\n",
    "        if len(sequence_index) >= DEFINES.max_sequence_length:\n",
    "            sequence_index = sequence_index[:DEFINES.max_sequence_length - 1] + [dictionary[END]]\n",
    "        else:\n",
    "            sequence_index += [dictionary[END]]\n",
    "        # max_sequence_length보다 문장 길이가\n",
    "        # 작다면 빈 부분에 PAD(0)를 넣어준다.\n",
    "        sequence_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        # 인덱스화 되어 있는 값을\n",
    "        # sequences_target_index에 넣어 준다.\n",
    "        sequences_target_index.append(sequence_index)\n",
    "    # 인덱스화된 일반 배열을 넘파이 배열로 변경한다.\n",
    "    # 이유는 텐서플로우 dataset에 넣어 주기 위한 사전 작업이다.\n",
    "    # 넘파이 배열에 인덱스화된 배열과 그 길이를 넘겨준다.\n",
    "    return np.asarray(sequences_target_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 스트링으로 변경하는 함수이다.\n",
    "# 바꾸고자 하는 인덱스 value와 인덱스를\n",
    "# 키로 가지고 있고 값으로 단어를 가지고 있는\n",
    "# 딕셔너리를 받는다.\n",
    "def pred2string(value, dictionary):\n",
    "    # 텍스트 문장을 보관할 배열을 선언한다.\n",
    "    sentence_string = []\n",
    "    print(value)\n",
    "    # 인덱스 배열 하나를 꺼내서 v에 넘겨준다.\n",
    "    for v in value:\n",
    "        # 딕셔너리에 있는 단어로 변경해서 배열에 담는다.\n",
    "        print(v['indexs'])\n",
    "        for index in v['indexs']:\n",
    "            print(index)\n",
    "        sentence_string = [dictionary[index] for index in v['indexs']]\n",
    "\n",
    "    print(\"***********************\")\n",
    "    print(sentence_string)\n",
    "    print(\"***********************\")\n",
    "    answer = \"\"\n",
    "    # 패딩값도 담겨 있으므로 패딩은 모두 스페이스 처리 한다.\n",
    "    for word in sentence_string:\n",
    "        if word not in PAD and word not in END:\n",
    "            answer += word\n",
    "            answer += \" \"\n",
    "    # 결과를 출력한다.\n",
    "    print(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_next_string(value, dictionary):\n",
    "    # 텍스트 문장을 보관할 배열을 선언한다.\n",
    "    sentence_string = []\n",
    "    is_finished = False\n",
    "\n",
    "    # 인덱스 배열 하나를 꺼내서 v에 넘겨준다.\n",
    "    for v in value:\n",
    "        # 딕셔너리에 있는 단어로 변경해서 배열에 담는다.\n",
    "        sentence_string = [dictionary[index] for index in v['indexs']]\n",
    "\n",
    "    answer = \"\"\n",
    "    # 패딩값도 담겨 있으므로 패딩은 모두 스페이스 처리 한다.\n",
    "    for word in sentence_string:\n",
    "        if word == END:\n",
    "            is_finished = True\n",
    "            break\n",
    "\n",
    "        if word != PAD and word != END:\n",
    "            answer += word\n",
    "            answer += \" \"\n",
    "\n",
    "    # 결과를 출력한다.\n",
    "    return answer, is_finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange(input, output, target):\n",
    "    features = {\"input\": input, \"output\": output}\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 들어가 배치 데이터를 만드는 함수이다.\n",
    "def train_input_fn(train_input_enc, train_output_dec, train_target_dec, batch_size):\n",
    "    # Dataset을 생성하는 부분으로써 from_tensor_slices부분은\n",
    "    # 각각 한 문장으로 자른다고 보면 된다.\n",
    "    # train_input_enc, train_output_dec, train_target_dec\n",
    "    # 3개를 각각 한문장으로 나눈다.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_input_enc, train_output_dec, train_target_dec))\n",
    "    # 전체 데이터를 썩는다.\n",
    "    dataset = dataset.shuffle(buffer_size=len(train_input_enc))\n",
    "    # 배치 인자 값이 없다면  에러를 발생 시킨다.\n",
    "    assert batch_size is not None, \"train batchSize must not be None\"\n",
    "    # from_tensor_slices를 통해 나눈것을\n",
    "    # 배치크기 만큼 묶어 준다.\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    # 데이터 각 요소에 대해서 rearrange 함수를\n",
    "    # 통해서 요소를 변환하여 맵으로 구성한다.\n",
    "    dataset = dataset.map(rearrange)\n",
    "    # repeat()함수에 원하는 에포크 수를 넣을수 있으면\n",
    "    # 아무 인자도 없다면 무한으로 이터레이터 된다.\n",
    "    dataset = dataset.repeat()\n",
    "    # make_one_shot_iterator를 통해 이터레이터를\n",
    "    # 만들어 준다.\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    # 이터레이터를 통해 다음 항목의 텐서\n",
    "    # 개체를 넘겨준다.\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가에 들어가 배치 데이터를 만드는 함수이다.\n",
    "def eval_input_fn(eval_input_enc, eval_output_dec, eval_target_dec, batch_size):\n",
    "    # Dataset을 생성하는 부분으로써 from_tensor_slices부분은\n",
    "    # 각각 한 문장으로 자른다고 보면 된다.\n",
    "    # eval_input_enc, eval_output_dec, eval_target_dec\n",
    "    # 3개를 각각 한문장으로 나눈다.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eval_input_enc, eval_output_dec, eval_target_dec))\n",
    "    # 전체 데이터를 섞는다.\n",
    "    dataset = dataset.shuffle(buffer_size=len(eval_input_enc))\n",
    "    # 배치 인자 값이 없다면  에러를 발생 시킨다.\n",
    "    assert batch_size is not None, \"eval batchSize must not be None\"\n",
    "    # from_tensor_slices를 통해 나눈것을\n",
    "    # 배치크기 만큼 묶어 준다.\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    # 데이터 각 요소에 대해서 rearrange 함수를\n",
    "    # 통해서 요소를 변환하여 맵으로 구성한다.\n",
    "    dataset = dataset.map(rearrange)\n",
    "    # repeat()함수에 원하는 에포크 수를 넣을수 있으면\n",
    "    # 아무 인자도 없다면 무한으로 이터레이터 된다.\n",
    "    # 평가이므로 1회만 동작 시킨다.\n",
    "    dataset = dataset.repeat(1)\n",
    "    # make_one_shot_iterator를 통해\n",
    "    # 이터레이터를 만들어 준다.\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    # 이터레이터를 통해 다음 항목의\n",
    "    # 텐서 개체를 넘겨준다.\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tokenizer(data):\n",
    "    # 토크나이징 해서 담을 배열 생성\n",
    "    words = []\n",
    "    for sentence in data:\n",
    "        # FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "        # 위 필터와 같은 값들을 정규화 표현식을\n",
    "        # 통해서 모두 \"\" 으로 변환 해주는 부분이다.\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "        for word in sentence.split():\n",
    "            words.append(word)\n",
    "    # 토그나이징과 정규표현식을 통해 만들어진\n",
    "    # 값들을 넘겨 준다.\n",
    "    return [word for word in words if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocabulary():\n",
    "    # 사전을 담을 배열 준비한다.\n",
    "    vocabulary_list = []\n",
    "    # 사전을 구성한 후 파일로 저장 진행한다.\n",
    "    # 그 파일의 존재 유무를 확인한다.\n",
    "    if (not (os.path.exists(DEFINES.vocabulary_path))):\n",
    "        # 이미 생성된 사전 파일이 존재하지 않으므로\n",
    "        # 데이터를 가지고 만들어야 한다.\n",
    "        # 그래서 데이터가 존재 하면 사전을 만들기 위해서\n",
    "        # 데이터 파일의 존재 유무를 확인한다.\n",
    "        if (os.path.exists(DEFINES.data_path)):\n",
    "            # 데이터가 존재하니 판단스를 통해서\n",
    "            # 데이터를 불러오자\n",
    "            data_df = pd.read_csv(DEFINES.data_path, encoding='utf-8')\n",
    "            # 판다스의 데이터 프레임을 통해서\n",
    "            # 질문과 답에 대한 열을 가져 온다.\n",
    "            question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "            if DEFINES.tokenize_as_morph:  # 형태소에 따른 토크나이져 처리\n",
    "                question = prepro_like_morphlized(question)\n",
    "                answer = prepro_like_morphlized(answer)\n",
    "            data = []\n",
    "            # 질문과 답변을 extend을\n",
    "            # 통해서 구조가 없는 배열로 만든다.\n",
    "            data.extend(question)\n",
    "            data.extend(answer)\n",
    "            # 토큰나이져 처리 하는 부분이다.\n",
    "            words = data_tokenizer(data)\n",
    "            # 공통적인 단어에 대해서는 모두\n",
    "            # 필요 없으므로 한개로 만들어 주기 위해서\n",
    "            # set해주고 이것들을 리스트로 만들어 준다.\n",
    "            words = list(set(words))\n",
    "            # 데이터 없는 내용중에 MARKER를 사전에\n",
    "            # 추가 하기 위해서 아래와 같이 처리 한다.\n",
    "            # 아래는 MARKER 값이며 리스트의 첫번째 부터\n",
    "            # 순서대로 넣기 위해서 인덱스 0에 추가한다.\n",
    "            # PAD = \"<PADDING>\"\n",
    "            # STD = \"<START>\"\n",
    "            # END = \"<END>\"\n",
    "            # UNK = \"<UNKNWON>\"\n",
    "            words[:0] = MARKER\n",
    "        # 사전을 리스트로 만들었으니 이 내용을\n",
    "        # 사전 파일을 만들어 넣는다.\n",
    "        with open(DEFINES.vocabulary_path, 'w', encoding='utf-8') as vocabulary_file:\n",
    "            for word in words:\n",
    "                vocabulary_file.write(word + '\\n')\n",
    "\n",
    "    # 사전 파일이 존재하면 여기에서\n",
    "    # 그 파일을 불러서 배열에 넣어 준다.\n",
    "    with open(DEFINES.vocabulary_path, 'r', encoding='utf-8') as vocabulary_file:\n",
    "        for line in vocabulary_file:\n",
    "            vocabulary_list.append(line.strip())\n",
    "\n",
    "    # 배열에 내용을 키와 값이 있는\n",
    "    # 딕셔너리 구조로 만든다.\n",
    "    char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
    "    # 두가지 형태의 키와 값이 있는 형태를 리턴한다.\n",
    "    # (예) 단어: 인덱스 , 인덱스: 단어)\n",
    "    return char2idx, idx2char, len(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocabulary(vocabulary_list):\n",
    "    # 리스트를 키가 단어이고 값이 인덱스인\n",
    "    # 딕셔너리를 만든다.\n",
    "    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n",
    "    # 리스트를 키가 인덱스이고 값이 단어인\n",
    "    # 딕셔너리를 만든다.\n",
    "    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
    "    # 두개의 딕셔너리를 넘겨 준다.\n",
    "    return char2idx, idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_data(self):\n",
    "    char2idx, idx2char, vocabulary_length = load_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "#from configs import DEFINES\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(inputs, eps=1e-6):\n",
    "    # LayerNorm(x + Sublayer(x))\n",
    "    feature_shape = inputs.get_shape()[-1:]\n",
    "    #  평균과 표준편차을 넘겨 준다.\n",
    "    mean = tf.keras.backend.mean(inputs, [-1], keepdims=True)\n",
    "    std = tf.keras.backend.std(inputs, [-1], keepdims=True)\n",
    "    beta = tf.get_variable(\"beta\", initializer=tf.zeros(feature_shape))\n",
    "    gamma = tf.get_variable(\"gamma\", initializer=tf.ones(feature_shape))\n",
    "\n",
    "    return gamma * (inputs - mean) / (std + eps) + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sublayer_connection(inputs, sublayer, dropout=0.2):\n",
    "    outputs = layer_norm(inputs + tf.keras.layers.Dropout(dropout)(sublayer))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(dim, sentence_length):\n",
    "    encoded_vec = np.array([pos/np.power(10000, 2*i/dim)\n",
    "                            for pos in range(sentence_length) for i in range(dim)])\n",
    "\n",
    "    encoded_vec[::2] = np.sin(encoded_vec[::2])\n",
    "    encoded_vec[1::2] = np.cos(encoded_vec[1::2])\n",
    "\n",
    "    return tf.constant(encoded_vec.reshape([sentence_length, dim]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.Model):\n",
    "    def __init__(self, num_units, heads, masked=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = heads\n",
    "        self.masked = masked\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\n",
    "        self.key_dense = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\n",
    "        self.value_dense = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(num_units)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value, masked=False):\n",
    "        key_seq_length = float(key.get_shape().as_list()[-1])\n",
    "        key = tf.transpose(key, perm=[0, 2, 1])\n",
    "        outputs = tf.matmul(query, key) / tf.sqrt(key_seq_length)\n",
    "\n",
    "        if masked:\n",
    "            diag_vals = tf.ones_like(outputs[0, :, :])\n",
    "            tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])\n",
    "\n",
    "            paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\n",
    "            outputs = tf.where(tf.equal(masks, 0), paddings, outputs)\n",
    "\n",
    "        attention_map = tf.nn.softmax(outputs)\n",
    "\n",
    "        return tf.matmul(attention_map, value)\n",
    "\n",
    "    def call(self, query, key, value):\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        query = tf.concat(tf.split(query, self.heads, axis=-1), axis=0)\n",
    "        key = tf.concat(tf.split(key, self.heads, axis=-1), axis=0)\n",
    "        value = tf.concat(tf.split(value, self.heads, axis=-1), axis=0)\n",
    "\n",
    "        attention_map = self.scaled_dot_product_attention(query, key, value, self.masked)\n",
    "\n",
    "        attn_outputs = tf.concat(tf.split(attention_map, self.heads, axis=0), axis=-1)\n",
    "        attn_outputs = self.dense(attn_outputs) \n",
    "\n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, model_dims, ffn_dims, attn_heads, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.self_attention = [MultiHeadAttention(model_dims, attn_heads) for _ in range(num_layers)]\n",
    "        self.position_feedforward = [PositionWiseFeedForward(ffn_dims, model_dims) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output_layer = None\n",
    "\n",
    "        for i, (s_a, p_f) in enumerate(zip(self.self_attention, self.position_feedforward)):\n",
    "            with tf.variable_scope('encoder_layer_' + str(i + 1)):\n",
    "                attention_layer = sublayer_connection(inputs, s_a(inputs, inputs, inputs))\n",
    "                output_layer = sublayer_connection(attention_layer, p_f(attention_layer))\n",
    "\n",
    "                inputs = output_layer\n",
    "\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, model_dims, ffn_dims, attn_heads, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.self_attention = [MultiHeadAttention(model_dims, attn_heads, masked=True) for _ in range(num_layers)]\n",
    "        self.encoder_decoder_attention = [MultiHeadAttention(model_dims, attn_heads) for _ in range(num_layers)]\n",
    "        self.position_feedforward = [PositionWiseFeedForward(ffn_dims, model_dims) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, inputs, encoder_outputs):\n",
    "        output_layer = None\n",
    "\n",
    "        for i, (s_a, ed_a, p_f) in enumerate(zip(self.self_attention, self.encoder_decoder_attention, self.position_feedforward)):\n",
    "            with tf.variable_scope('decoder_layer_' + str(i + 1)):\n",
    "                masked_attention_layer = sublayer_connection(inputs, s_a(inputs, inputs, inputs))\n",
    "                attention_layer = sublayer_connection(masked_attention_layer, ed_a(masked_attention_layer,\n",
    "                                                                                           encoder_outputs,\n",
    "                                                                                           encoder_outputs))\n",
    "                output_layer = sublayer_connection(attention_layer, p_f(attention_layer))\n",
    "                inputs = output_layer\n",
    "\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.Model):\n",
    "    def __init__(self, num_units, feature_shape):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.inner_dense = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)\n",
    "        self.output_dense = tf.keras.layers.Dense(feature_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inner_layer = self.inner_dense(inputs)\n",
    "        outputs = self.output_dense(inner_layer)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(features, labels, mode, params):\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "\n",
    "    position_encode = positional_encoding(params['embedding_size'], params['max_sequence_length'])\n",
    "\n",
    "    embedding = tf.keras.layers.Embedding(params['vocabulary_length'],\n",
    "                                          params['embedding_size'])\n",
    "\n",
    "    encoder_layers = Encoder(params['model_hidden_size'], params['ffn_hidden_size'],\n",
    "                      params['attention_head_size'], params['layer_size'])\n",
    "\n",
    "    decoder_layers = Decoder(params['model_hidden_size'], params['ffn_hidden_size'],\n",
    "                      params['attention_head_size'], params['layer_size'])\n",
    "\n",
    "    logit_layer = tf.keras.layers.Dense(params['vocabulary_length'])\n",
    "\n",
    "    with tf.variable_scope('encoder', reuse=tf.AUTO_REUSE):\n",
    "        x_embedded_matrix = embedding(features['input']) + position_encode\n",
    "        encoder_outputs = encoder_layers(x_embedded_matrix)\n",
    "\n",
    "    loop_count = params['max_sequence_length'] if PREDICT else 1\n",
    "\n",
    "    predict, output, logits = None, None, None\n",
    "\n",
    "    for i in range(loop_count):\n",
    "        with tf.variable_scope('decoder', reuse=tf.AUTO_REUSE):\n",
    "            if i > 0:\n",
    "                output = tf.concat([tf.ones((output.shape[0], 1), dtype=tf.int64), predict[:, :-1]], axis=-1)\n",
    "            else:\n",
    "                output = features['output']\n",
    "\n",
    "            y_embedded_matrix = embedding(output) + position_encode\n",
    "            decoder_outputs = decoder_layers(y_embedded_matrix, encoder_outputs)\n",
    "\n",
    "            logits = logit_layer(decoder_outputs)\n",
    "            predict = tf.argmax(logits, 2)\n",
    "\n",
    "    if PREDICT:\n",
    "        predictions = {\n",
    "            'indexs': predict,\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predict, name='accOp')\n",
    "\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    if EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    assert TRAIN\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#import model as ml\n",
    "#import data\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#from configs import DEFINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_out/'\n",
    "data_out_path = os.path.join(os.getcwd(), DATA_OUT_PATH)\n",
    "os.makedirs(data_out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 통한 사전 구성 한다.\n",
    "char2idx, idx2char, vocabulary_length = load_vocabulary() #data.load_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터와 테스트 데이터를 가져온다.\n",
    "train_input, train_label, eval_input, eval_label = load_data() # data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련셋 인코딩 만드는 부분이다.\n",
    "train_input_enc, train_input_enc_length = enc_processing(train_input, char2idx) #data.enc_processing()\n",
    "# 훈련셋 디코딩 입력 부분 만드는 부분이다.\n",
    "train_output_dec, train_output_dec_length = dec_output_processing(train_label, char2idx) #data.dec_output_processing()\n",
    "# 훈련셋 디코딩 출력 부분 만드는 부분이다.\n",
    "train_target_dec = dec_target_processing(train_label, char2idx) #data.dec_target_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가셋 인코딩 만드는 부분이다.\n",
    "eval_input_enc, eval_input_enc_length = enc_processing(eval_input, char2idx) #data.enc_processing()\n",
    "# 평가셋 인코딩 만드는 부분이다.\n",
    "eval_output_dec, eval_output_dec_length = dec_output_processing(eval_label, char2idx) #data.dec_output_processing()\n",
    "# 평가셋 인코딩 만드는 부분이다.\n",
    "eval_target_dec = dec_target_processing(eval_label, char2idx) #data.dec_target_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 경로'./'에 현재 경로 하부에\n",
    "# 체크 포인트를 저장한 디렉토리를 설정한다.\n",
    "check_point_path = os.path.join(os.getcwd(), DEFINES.check_point_path)\n",
    "# 디렉토리를 만드는 함수이며 두번째 인자 exist_ok가\n",
    "# True이면 디렉토리가 이미 존재해도 OSError가\n",
    "# 발생하지 않는다.\n",
    "# exist_ok가 False이면 이미 존재하면\n",
    "# OSError가 발생한다.\n",
    "os.makedirs(check_point_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './data_out/check_point', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a2fca1790>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# 에스티메이터 구성한다.\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=Model,  # 모델 등록한다. #ml.Model,\n",
    "    model_dir=DEFINES.check_point_path,  # 체크포인트 위치 등록한다.\n",
    "    params={  # 모델 쪽으로 파라메터 전달한다.\n",
    "        'embedding_size': DEFINES.embedding_size,\n",
    "        'model_hidden_size': DEFINES.model_hidden_size,  # 가중치 크기 설정한다.\n",
    "        'ffn_hidden_size': DEFINES.ffn_hidden_size,\n",
    "        'attention_head_size': DEFINES.attention_head_size,\n",
    "        'learning_rate': DEFINES.learning_rate,  # 학습율 설정한다.\n",
    "        'vocabulary_length': vocabulary_length,  # 딕셔너리 크기를 설정한다.\n",
    "        'embedding_size': DEFINES.embedding_size,  # 임베딩 크기를 설정한다.\n",
    "        'layer_size': DEFINES.layer_size,\n",
    "        'max_sequence_length': DEFINES.max_sequence_length,\n",
    "        'xavier_initializer': DEFINES.xavier_initializer\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./data_out/check_point/model.ckpt.\n",
      "INFO:tensorflow:loss = 9.782045, step = 1\n",
      "INFO:tensorflow:global_step/sec: 0.684345\n",
      "INFO:tensorflow:loss = 1.4048139, step = 101 (146.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.901501\n",
      "INFO:tensorflow:loss = 1.2672542, step = 201 (110.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.76821\n",
      "INFO:tensorflow:loss = 1.215184, step = 301 (130.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.888169\n",
      "INFO:tensorflow:loss = 1.0068612, step = 401 (112.589 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 487 into ./data_out/check_point/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.890447\n",
      "INFO:tensorflow:loss = 1.1383394, step = 501 (112.303 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.857505\n",
      "INFO:tensorflow:loss = 1.0561926, step = 601 (116.619 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.899892\n",
      "INFO:tensorflow:loss = 0.85345507, step = 701 (111.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.880725\n",
      "INFO:tensorflow:loss = 0.7850611, step = 801 (113.535 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.811231\n",
      "INFO:tensorflow:loss = 0.7418835, step = 901 (123.271 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-f64d72c51f4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# data.train_input_fn()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m classifier.train(input_fn=lambda: train_input_fn(\n\u001b[0;32m----> 4\u001b[0;31m     train_input_enc, train_output_dec, train_target_dec, DEFINES.batch_size), steps=DEFINES.train_steps)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1158\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1192\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1193\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1491\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1493\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1494\u001b[0m         \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m             run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1260\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m         logging.info(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습 실행\n",
    "# data.train_input_fn()\n",
    "classifier.train(input_fn=lambda: train_input_fn(\n",
    "    train_input_enc, train_output_dec, train_target_dec, DEFINES.batch_size), steps=DEFINES.train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INFO:tensorflow:Calling model_fn.\n",
    "INFO:tensorflow:Done calling model_fn.\n",
    "INFO:tensorflow:Create CheckpointSaverHook.\n",
    "INFO:tensorflow:Graph was finalized.\n",
    "INFO:tensorflow:Running local_init_op.\n",
    "INFO:tensorflow:Done running local_init_op.\n",
    "INFO:tensorflow:Saving checkpoints for 0 into ./data_out/check_point/model.ckpt.\n",
    "INFO:tensorflow:loss = 9.782045, step = 1\n",
    "INFO:tensorflow:global_step/sec: 0.684345\n",
    "INFO:tensorflow:loss = 1.4048139, step = 101 (146.134 sec)\n",
    "INFO:tensorflow:global_step/sec: 0.901501\n",
    "INFO:tensorflow:loss = 1.2672542, step = 201 (110.918 sec)\n",
    "INFO:tensorflow:global_step/sec: 0.76821\n",
    "INFO:tensorflow:loss = 1.215184, step = 301 (130.175 sec)\n",
    "INFO:tensorflow:global_step/sec: 0.888169\n",
    "INFO:tensorflow:loss = 1.0068612, step = 401 (112.589 sec)\n",
    "INFO:tensorflow:Saving checkpoints for 487 into ./data_out/check_point/model.ckpt.\n",
    "INFO:tensorflow:global_step/sec: 0.890447\n",
    "INFO:tensorflow:loss = 1.1383394, step = 501 (112.303 sec)\n",
    "INFO:tensorflow:global_step/sec: 0.857505\n",
    "INFO:tensorflow:loss = 1.0561926, step = 601 (116.619 sec)\n",
    "INFO:tensorflow:global_step/sec: 0.899892\n",
    "INFO:tensorflow:loss = 0.85345507, step = 701 (111.131 sec)\n",
    "INFO:tensorflow:global_step/sec: 0.880725\n",
    "INFO:tensorflow:loss = 0.7850611, step = 801 (113.535 sec)\n",
    "INFO:tensorflow:global_step/sec: 0.811231\n",
    "INFO:tensorflow:loss = 0.7418835, step = 901 (123.271 sec)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 실행\n",
    "# data.eval_input_fn()\n",
    "eval_result = classifier.evaluate(input_fn=lambda: eval_input_fn(\n",
    "    eval_input_enc, eval_output_dec, eval_target_dec, DEFINES.batch_size))\n",
    "print('\\nEVAL set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트용 데이터 만드는 부분이다.\n",
    "# 인코딩 부분 만든다.\n",
    "predic_input_enc, predic_input_enc_length = data.enc_processing([\"가끔 궁금해\"], char2idx)\n",
    "# 학습 과정이 아니므로 디코딩 입력은\n",
    "# 존재하지 않는다.(구조를 맞추기 위해 넣는다.)\n",
    "predic_output_dec, predic_output_decLength = data.dec_output_processing([\"\"], char2idx)\n",
    "# 학습 과정이 아니므로 디코딩 출력 부분도\n",
    "# 존재하지 않는다.(구조를 맞추기 위해 넣는다.)\n",
    "predic_target_dec = data.dec_target_processing([\"\"], char2idx)\n",
    "\n",
    "predictions = classifier.predict(\n",
    "    input_fn=lambda: data.eval_input_fn(predic_input_enc, predic_output_dec, predic_target_dec, 1))\n",
    "\n",
    "answer, finished = data.pred_next_string(predictions, idx2char)\n",
    "\n",
    "# 예측한 값을 인지 할 수 있도록\n",
    "# 텍스트로 변경하는 부분이다.\n",
    "print(\"answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#import tensorflow as tf\n",
    "#import data\n",
    "import os\n",
    "import sys\n",
    "#import model as ml\n",
    "\n",
    "#from configs import DEFINES\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    arg_length = len(sys.argv)\n",
    "\n",
    "    if (arg_length < 2):\n",
    "        raise Exception(\"Don't call us. We'll call you\")\n",
    "\n",
    "    # 데이터를 통한 사전 구성 한다.\n",
    "    char2idx, idx2char, vocabulary_length = data.load_vocabulary()\n",
    "\n",
    "    # 테스트용 데이터 만드는 부분이다.\n",
    "    # 인코딩 부분 만든다.\n",
    "    input = \"\"\n",
    "    for i in sys.argv[1:]:\n",
    "        input += i\n",
    "        input += \" \"\n",
    "\n",
    "    print(input)\n",
    "    predic_input_enc, predic_input_enc_length = data.enc_processing([input], char2idx)\n",
    "    # 학습 과정이 아니므로 디코딩 입력은\n",
    "    # 존재하지 않는다.(구조를 맞추기 위해 넣는다.)\n",
    "    predic_output_dec, predic_output_dec_length = data.dec_output_processing([\"\"], char2idx)\n",
    "    # 학습 과정이 아니므로 디코딩 출력 부분도\n",
    "    # 존재하지 않는다.(구조를 맞추기 위해 넣는다.)\n",
    "    predic_target_dec = data.dec_target_processing([\"\"], char2idx)\n",
    "\n",
    "    # 에스티메이터 구성한다.\n",
    "    classifier = tf.estimator.Estimator(\n",
    "        model_fn=ml.Model,  # 모델 등록한다.\n",
    "        model_dir=DEFINES.check_point_path,  # 체크포인트 위치 등록한다.\n",
    "        params={  # 모델 쪽으로 파라메터 전달한다.\n",
    "            'embedding_size': DEFINES.embedding_size,\n",
    "            'model_hidden_size': DEFINES.model_hidden_size,  # 가중치 크기 설정한다.\n",
    "            'ffn_hidden_size': DEFINES.ffn_hidden_size,\n",
    "            'attention_head_size': DEFINES.attention_head_size,\n",
    "            'learning_rate': DEFINES.learning_rate,  # 학습율 설정한다.\n",
    "            'vocabulary_length': vocabulary_length,  # 딕셔너리 크기를 설정한다.\n",
    "            'embedding_size': DEFINES.embedding_size,  # 임베딩 크기를 설정한다.\n",
    "            'layer_size': DEFINES.layer_size,\n",
    "            'max_sequence_length': DEFINES.max_sequence_length,\n",
    "            'xavier_initializer': DEFINES.xavier_initializer\n",
    "        })\n",
    "\n",
    "    # for i in range(DEFINES.max_sequence_length):\n",
    "    #     if i > 0:\n",
    "    #         predic_output_dec, predic_output_decLength = data.dec_output_processing([answer], char2idx)\n",
    "    #         predic_target_dec = data.dec_target_processing([answer], char2idx)\n",
    "    #     # 예측을 하는 부분이다.\n",
    "    #     predictions = classifier.predict(\n",
    "    #         input_fn=lambda: data.eval_input_fn(predic_input_enc, predic_output_dec, predic_target_dec, 1))\n",
    "    #\n",
    "    #     answer, finished = data.pred_next_string(predictions, idx2char)\n",
    "    #     print(answer)\n",
    "    #     if finished:\n",
    "    #         break\n",
    "\n",
    "    # predic_output_dec, predic_output_dec_length = data.dec_output_processing([\"\"], char2idx)\n",
    "    # predic_target_dec = data.dec_target_processing([\"\"], char2idx)\n",
    "    # 예측을 하는 부분이다.\n",
    "    predictions = classifier.predict(input_fn=lambda: data.eval_input_fn(predic_input_enc, predic_output_dec, predic_target_dec, 1))\n",
    "\n",
    "    answer, finished = data.pred_next_string(predictions, idx2char)\n",
    "\n",
    "    # 예측한 값을 인지 할 수 있도록\n",
    "    # 텍스트로 변경하는 부분이다.\n",
    "    print(\"answer: \", answer)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
